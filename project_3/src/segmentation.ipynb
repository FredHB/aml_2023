{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from fastai.vision.all import *\n",
    "from fastcore.xtras import Path\n",
    "\n",
    "# from fastai.callback.hook import summary\n",
    "# from fastai.callback.progress import ProgressCallback\n",
    "# from fastai.callback.schedule import lr_find, fit_flat_cos\n",
    "\n",
    "# from fastai.data.block import DataBlock\n",
    "# from fastai.data.external import untar_data, URLs\n",
    "# from fastai.data.transforms import get_image_files, Normalize, FuncSplitter\n",
    "\n",
    "# from fastai.losses import BaseLoss\n",
    "# from fastai.layers import Mish\n",
    "# from fastai.optimizer import ranger\n",
    "\n",
    "# from fastai.torch_core import tensor\n",
    "\n",
    "# from fastai.vision.augment import aug_transforms\n",
    "# from fastai.vision.core import PILImage, PILMask, Image\n",
    "\n",
    "# from fastai.vision.data import ImageBlock, MaskBlock, imagenet_stats\n",
    "# from fastai.vision.learner import unet_learner\n",
    "\n",
    "# from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "# from torchvision.transforms import ToPILImage\n",
    "# from torchvision.models.resnet import resnet34\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from numba import jit, njit, prange\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * # own utilities script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/processed/')\n",
    "train_data, test_data, samples = get_pkl_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that attaches four frames in one single image for the entire video\n",
    "def make_three_channel(video):\n",
    "    vid_sz = video.shape[0:2]\n",
    "    num_frames = video.shape[2]\n",
    "\n",
    "    framelist = []\n",
    "    for f in range(num_frames):\n",
    "        frame = np.zeros((vid_sz[0], vid_sz[1], 3), dtype=video.dtype)\n",
    "        frame[:,:,0] = video[:, :, f]\n",
    "        if f+1 < num_frames:\n",
    "            frame[:,:,1] = video[:, :, f+1]\n",
    "        if f+2 < num_frames:\n",
    "            frame[:,:,2] = video[:, :, f+2]\n",
    "        framelist.append(frame)\n",
    "\n",
    "    frames = np.stack(framelist, axis=3)  \n",
    "    return frames\n",
    "\n",
    "# function to transform data to trichannel frames\n",
    "def trichannel_frames(data):\n",
    "    with tqdm(total=len(data)) as pbar:\n",
    "        for id in range(len(data)):\n",
    "            video = make_three_channel(data[id]['video'])\n",
    "            data[id]['video'] = video\n",
    "            pbar.update(1)\n",
    "    return data\n",
    "\n",
    "def sharpen_and_brighten(video):\n",
    "    # Assuming `video` is your 3D array of frames\n",
    "    sharpened_video = np.empty_like(video)\n",
    "\n",
    "    # Define the Laplacian sharpening kernel\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)\n",
    "\n",
    "    for i in range(video.shape[2]):\n",
    "        frame = video[:, :, i]\n",
    "        \n",
    "        # Apply the sharpening filter\n",
    "        sharpened_video[:, :, i] = cv2.equalizeHist(cv2.filter2D(frame, -1, kernel))\n",
    "\n",
    "    return sharpened_video\n",
    "\n",
    "def brighten(video):\n",
    "    # Assuming `video` is your 3D array of frames\n",
    "    brightened_video = np.empty_like(video)\n",
    "\n",
    "    for i in range(video.shape[2]):\n",
    "        frame = video[:, :, i]\n",
    "        \n",
    "        # Apply the sharpening filter\n",
    "        brightened_video[:, :, i] = cv2.equalizeHist(frame)\n",
    "\n",
    "    return brightened_video\n",
    "\n",
    "def normalize_light_data(data):\n",
    "    with tqdm(total=len(data)) as pbar:\n",
    "        for id in range(len(data)):\n",
    "            video = brighten(data[id]['video'])\n",
    "            data[id]['video'] = video\n",
    "            pbar.update(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trichannel_frames(train_data)\n",
    "test_data = trichannel_frames(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# function that attaches four frames in one single image for the entire video\n",
    "def attach_four_frames(vid_list, train=True):\n",
    "\n",
    "    video = vid_list[0]\n",
    "    if train:\n",
    "        videolab = vid_list[1]\n",
    "        \n",
    "    vid_sz = video.shape[0:2]\n",
    "    num_frames = video.shape[2]\n",
    "\n",
    "    framelist = []\n",
    "    with tqdm(total=num_frames+train*num_frames) as pbar:\n",
    "        for f in range(num_frames):\n",
    "            frame = np.zeros((vid_sz[0]*2, vid_sz[1]*2), dtype=video.dtype)\n",
    "            frame[0:vid_sz[0], 0:vid_sz[1]] = video[:, :, f]\n",
    "            if f+1 < num_frames:\n",
    "                frame[0:vid_sz[0], vid_sz[1]:2*vid_sz[1]] = video[:, :, f+1]\n",
    "            if f+2 < num_frames:\n",
    "                frame[vid_sz[0]:2*vid_sz[0], 0:vid_sz[1]] = video[:, :, f+2]\n",
    "            if f+3 < num_frames:\n",
    "                frame[vid_sz[0]:2*vid_sz[0], vid_sz[1]:2*vid_sz[1]] = video[:, :, f+3]\n",
    "            framelist.append(frame)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        frames = np.dstack(framelist)\n",
    "        \n",
    "        if train:\n",
    "            labellist = []\n",
    "            for f in range(num_frames):\n",
    "                label = np.zeros((vid_sz[0]*2, vid_sz[1]*2), dtype=videolab.dtype)\n",
    "                label[0:vid_sz[0], 0:vid_sz[1]] = videolab[:, :, f]            \n",
    "                labellist.append(label)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            labels = np.dstack(labellist)\n",
    "            return frames, labels\n",
    "        else:\n",
    "            return frames\n",
    "\n",
    "# function to transform data to quad frames\n",
    "def quad_frames(data, train):\n",
    "    with tqdm(total=len(data)) as pbar:\n",
    "        for id in range(len(data)):\n",
    "            if train:\n",
    "                frames, labels = attach_four_frames([data[id]['video'], data[id]['label']], train=train)\n",
    "                data[id]['label'] = labels\n",
    "            else:\n",
    "                frames = attach_four_frames([data[id]['video']], train=train)\n",
    "            data[id]['video'] = frames\n",
    "            pbar.update(1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make png scan files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_pngs(train_data, test_data, data_path, trichannel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make png label files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_label_pngs(train_data, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function that splits the training data into a training and validation set. You can use the `random.sample` function for this. Split along ids, and allow an option to include amateur data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fnames, valid_fnames = get_sample_split_txt(0.1, 'full', data_path, 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from utils import *\n",
    "\n",
    "data_path = Path('../data/processed/')\n",
    "path_im = data_path/'train'/'scans'\n",
    "path_lbl = data_path/'train'/'labels'\n",
    "fnames = get_image_files(path_im)\n",
    "lbl_names = get_image_files(path_lbl)\n",
    "\n",
    "# provide path to an image --> returns path to the mask\n",
    "get_mask = lambda o: path_lbl/f'{o.stem}_lab{o.suffix}'\n",
    "\n",
    "# codes for each segmentation class\n",
    "codes = np.array(['BG', 'MV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out an image\n",
    "PILImage.create(fnames[0]).show(figsize=(2,2), title=\"Scan\")\n",
    "\n",
    "# check out a label\n",
    "print(\"The mask for\", fnames[0], \"is:\\n\", get_mask(fnames[0]))\n",
    "\n",
    "msk = PILMask.create(get_mask(fnames[0]))\n",
    "msk.show(figsize=(2,2), alpha=1, title=\"Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try FileSplitter function\n",
    "FileSplitter(data_path/'train'/'vld_expert.txt')(fnames) # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the resolution of an expert and an amateur frame\n",
    "res_exp, res_am = get_resolution(fnames, data_path)\n",
    "half_res = (int(res_exp[0]/2), int(res_exp[1]/2)); \n",
    "double_am = (int(res_am[0]*2), int(res_am[1]*2))\n",
    "trip_am = (int(res_am[0]*3), int(res_am[1]*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvscans = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), # blocks for segmentation\n",
    "                    get_items=get_image_files, # how to get the files: use function\n",
    "                    splitter=FileSplitter(data_path/'train'/'vld_full.txt'), # function to split the files\n",
    "                    get_y=get_mask,\n",
    "                    item_tfms=Resize(double_am),\n",
    "                    batch_tfms=[*aug_transforms(size=double_am, \n",
    "                                                do_flip=False, \n",
    "                                                max_rotate=0.,\n",
    "                                                max_zoom=1.0,\n",
    "                                                max_warp=0.,\n",
    "                                                p_affine=0.), Normalize.from_stats(*imagenet_stats)]\n",
    "                    )\n",
    "\n",
    "dls = mvscans.dataloaders(path_im, bs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a batch of images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n = 4, vmin=0, vmax=1, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,3, figsize=(12,3))\n",
    "amnames = fnames.filter(lambda x: 'am' in x.stem).map(lambda x: x.stem).filter(lambda x: f'am_{amid}_' in x)\n",
    "\n",
    "PILImage.create(\"../data/processed/train/scans/\"+amnames[0]+\".png\").show( title=amnames[0], ctx=axs[0])\n",
    "PILMask.create(\"../data/processed/train/labels/\"+amnames[0]+\"_lab.png\").show( ctx=axs[0])\n",
    "PILImage.create(\"../data/processed/train/scans/\"+amnames[1]+\".png\").show( title=amnames[1], ctx=axs[1])\n",
    "PILMask.create(\"../data/processed/train/labels/\"+amnames[1]+\"_lab.png\").show( ctx=axs[1])\n",
    "PILImage.create(\"../data/processed/train/scans/\"+amnames[2]+\".png\").show( title=amnames[2], ctx=axs[2])\n",
    "PILMask.create(\"../data/processed/train/labels/\"+amnames[2]+\"_lab.png\").show( ctx=axs[2])\n",
    "\n",
    "print(amnames)\n",
    "\n",
    "amid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.vocab = codes\n",
    "name2id = {v:k for k,v in enumerate(codes)}; name2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ranger\n",
    "learn = unet_learner(\n",
    "    dls, # dataloaders\n",
    "    resnet34, # architecture\n",
    "    metrics=acc_camvid, # metric\n",
    "    self_attention=True,\n",
    "    act_cls=Mish,\n",
    "    opt_func=opt\n",
    ")\n",
    "\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # set learning rate\n",
    "learn.fit_flat_cos(10, slice(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = Path('../out/models/')\n",
    "path_models.mkdir(exist_ok=True, parents=True)\n",
    "learn.save('../../out/models/full_sample_extended_tric_224_10ep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some results. They are not so bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=4, figsize=(6,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Size Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from utils import *\n",
    "\n",
    "data_path = Path('../data/processed/')\n",
    "path_im = data_path/'train'/'scans'\n",
    "path_lbl = data_path/'train'/'labels'\n",
    "fnames = get_image_files(path_im)\n",
    "lbl_names = get_image_files(path_lbl)\n",
    "\n",
    "# provide path to an image --> returns path to the mask\n",
    "get_mask = lambda o: path_lbl/f'{o.stem}_lab{o.suffix}'\n",
    "\n",
    "# codes for each segmentation class\n",
    "codes = np.array(['BG', 'MV'])\n",
    "\n",
    "# validation filesnames\n",
    "valid_fnames = (data_path/'train'/'vld_full.txt').read_text().split('\\n')\n",
    "\n",
    "res_exp, res_am = get_resolution(fnames, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_res = (int(res_exp[1]/2), int(res_exp[0]/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dataloaders with full size images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvscans = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), # blocks for segmentation\n",
    "                    get_items=get_image_files, # how to get the files: use function\n",
    "                    splitter=FileSplitter(data_path/'train'/'vld_full.txt'), # function to split the files\n",
    "                    get_y=get_mask,\n",
    "                    item_tfms=Resize(half_res),\n",
    "                    batch_tfms=[*aug_transforms(size=half_res), Normalize.from_stats(*imagenet_stats)]\n",
    "                    )\n",
    "\n",
    "dls = mvscans.dataloaders(path_im, bs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign vocab, make learner, load weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ranger\n",
    "dls.vocab = codes\n",
    "learn = unet_learner(\n",
    "    dls, # dataloaders\n",
    "    resnet34, # architecture\n",
    "    metrics=acc_camvid, # metric\n",
    "    self_attention=True,\n",
    "    act_cls=Mish,\n",
    "    opt_func=opt\n",
    ")\n",
    "learn.load('../../out/models/full_sample_lowres_10ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for ten epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(10, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dls full size\n",
    "mvscans = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), # blocks for segmentation\n",
    "                    get_items=get_image_files, # how to get the files: use function\n",
    "                    splitter=FileSplitter(data_path/'train'/'vld_full.txt'), # function to split the files\n",
    "                    get_y=get_mask,\n",
    "                    item_tfms=Resize(res_exp),\n",
    "                    batch_tfms=[*aug_transforms(size=res_exp), Normalize.from_stats(*imagenet_stats)]\n",
    "                    )\n",
    "\n",
    "dls_full = mvscans.dataloaders(path_im, bs=1)\n",
    "dls_full.vocab = codes\n",
    "learn_full = unet_learner(\n",
    "    dls_full, # dataloaders\n",
    "    resnet34, # architecture\n",
    "    metrics=acc_camvid, # metric\n",
    "    self_attention=True,\n",
    "    act_cls=Mish,\n",
    "    opt_func=opt\n",
    ")\n",
    "\n",
    "learn_full.fit_flat_cos(5, slice(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('../../out/models/full_sample_highres_10ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "lrs = slice(1e-6,lr/10)\n",
    "learn.fit_flat_cos(10, lrs)\n",
    "learn.save('../../out/models/full_sample_highres_10ep_unfreeze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Size Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_inference = (224, 224)\n",
    "# load the model and a dataloader with the correct image size\n",
    "mvscans = DataBlock(blocks=(ImageBlock, MaskBlock(codes)), # blocks for segmentation\n",
    "                    get_items=get_image_files, # how to get the files: use function\n",
    "                    splitter=FileSplitter(data_path/'train'/'vld_full.txt'), # function to split the files\n",
    "                    get_y=get_mask,\n",
    "                    item_tfms=[Resize(res_inference)],\n",
    "                    batch_tfms=[*aug_transforms(size=res_inference, \n",
    "                                                do_flip=False, \n",
    "                                                max_rotate=0.,\n",
    "                                                max_zoom=1.0,\n",
    "                                                max_warp=0.,\n",
    "                                                p_affine=0.), Normalize.from_stats(*imagenet_stats)]\n",
    "                    )\n",
    "\n",
    "dls = mvscans.dataloaders(path_im, bs=4)\n",
    "learn_pred = unet_learner(\n",
    "    dls, # dataloaders\n",
    "    resnet34, # architecture\n",
    "    metrics=acc_camvid, # metric\n",
    "    self_attention=True,\n",
    "    act_cls=Mish,\n",
    "    opt_func=ranger\n",
    ")\n",
    "learn_pred.load('../../out/models/full_sample_extended_tric_224_10ep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_mask_to_frame(msk_array, dim_frame):\n",
    "    \"\"\"Takes a square mask array and frame dimensions and returns a mask with the same shape as the video frame\"\"\"\n",
    "    dim_msk = msk_array.shape[0]\n",
    "    scale_y = dim_frame[0]/dim_msk\n",
    "    rsz = lambda o: CropPad(dim_frame)(RatioResize(scale_y*dim_msk)(o))\n",
    "    msk = PILMask.create(msk_array)\n",
    "    return tensor(rsz(msk))\n",
    "\n",
    "\n",
    "def show_some_predictions(preds, test_data, ids, frames, threshold = 0.5,  k = -1):\n",
    "    \"\"\"\n",
    "        Shape of predictions must be square.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    rsz = Resize(res_inference[0], method=ResizeMethod.Crop)\n",
    "    \n",
    "    if k == -1 :\n",
    "        k = random.randint(0, len(preds[0]-5))\n",
    "\n",
    "    print(\"Index of predictions: \", k, \", ..., \", k+5)\n",
    "    for i in range(k, k+6) :\n",
    "        pred_array = preds[0][i][1] > threshold # a threshold of 0.2 is applied to the prediction\n",
    "                \n",
    "        scan = test_data[ids[i]]['video'][:,:,frames[i]]\n",
    "        img = PILImage.create(scan)\n",
    "        \n",
    "        msk = PILMask.create(pred_array)\n",
    "        scale_y = img.shape[0]/scan.shape[0]\n",
    "        rsz = lambda o: CropPad(img.shape)(RatioResize(scale_y*scan.shape[0])(o))\n",
    "\n",
    "        axs[(i-k)//3, (i-k)%3].imshow(img, cmap='gray', alpha=1)\n",
    "        axs[(i-k)//3, (i-k)%3].imshow(tensor(rsz(msk)), alpha=0.5)\n",
    "        axs[(i-k)//3, (i-k)%3].set_title(f\"Image ID {ids[i]}, Frame {frames[i]}\")\n",
    "        axs[(i-k)//3, (i-k)%3].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predictions2list(preds, test_data, threshold=0.5):\n",
    "    \"\"\"Takes a list of predictions, the test_data and a threshold for classification, and returns \n",
    "        \n",
    "        a list of dictionaries with \n",
    "        \n",
    "        - name of the video\n",
    "        - rescaled prediction\n",
    "        \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    list_predictions = []\n",
    "    while i < len(preds[0]):\n",
    "\n",
    "        id = ids[i]\n",
    "        frames_id = frames[np.array(ids) == id]\n",
    "\n",
    "        list_frames = []\n",
    "        for frame in frames_id:\n",
    "\n",
    "            dim_frame = test_data[id]['video'][:,:,frame].shape\n",
    "            msk_array = preds[0][i][1, :, :]\n",
    "\n",
    "            msk_array_rszd = shape_mask_to_frame(msk_array, dim_frame).numpy()\n",
    "            msk_array_rszd = msk_array_rszd > threshold\n",
    "            list_frames.append(msk_array_rszd)\n",
    "            i += 1\n",
    "\n",
    "        dict_i = {\n",
    "            \"name\": test_data[id]['name'],\n",
    "            \"prediction\": np.dstack(list_frames)\n",
    "        }\n",
    "        list_predictions.append(dict_i)\n",
    "    \n",
    "    return list_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files for prediction, associated ids and frames\n",
    "pred_list = get_image_files(data_path/'test'/'scans').map(lambda o: o.stem)\n",
    "pred_list = L(sorted(pred_list, key=lambda o: 1e6*int(o.split(\"_\")[1]) + int(o.split(\"_\")[2]))) # sort by id and frame\n",
    "\n",
    "#all predictions\n",
    "#pred_list = pred_list[0:300] # first 300 predictions\n",
    "\n",
    "ids = pred_list.map(lambda o: int(o.split(\"_\")[1]))\n",
    "frames = pred_list.map(lambda o: int(o.split(\"_\")[2]))\n",
    "\n",
    "dl_test = learn_pred.dls.test_dl([data_path/'test'/('scans/' + pred_list[i] + '.png') for i in range(len(pred_list))])\n",
    "\n",
    "dl_test.show_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = learn_pred.get_preds(dl=dl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The resolution of the prediction is (are): ', L([preds[0][i].argmax(dim=0).numpy().shape for i in range(len(preds[0]))]).unique() )\n",
    "\n",
    "# load test images\n",
    "_, test_data, _ = get_pkl_data(data_path)\n",
    "\n",
    "print('The resolutions of the test data are:', L([test_data[ids[i]]['video'][:,:,frames[i]].shape for i in range(len(ids))]).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too dark: 14 (*2)\n",
    "18 (*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 9\n",
    "\n",
    "print('avg light:', test_data[id]['video'].mean())\n",
    "frame = test_data[id]['video'][:,:,f]*1.\n",
    "frame[frame > 255] = 255\n",
    "plt.imshow(frame, cmap='gray')\n",
    "f += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to training set\n",
    "# from predictions of \"full_sample_noval_tric_224_15ep\"\n",
    "\n",
    "add_list = [(11, 74),\n",
    "(11, 84),\n",
    "(12, 0),\n",
    "(12, 19),\n",
    "(12, 27),\n",
    "(12, 48),\n",
    "(12, 56),\n",
    "(13, 8),\n",
    "(13, 11),\n",
    "(13, 15),\n",
    "(13, 24),\n",
    "(13, 26),\n",
    "(14, 18),\n",
    "(14, 30),\n",
    "(14, 37),\n",
    "(15, 15),\n",
    "(15, 23),\n",
    "(15, 29),\n",
    "(15, 39),\n",
    "(15, 63),\n",
    "(16, 5),\n",
    "(16, 15),\n",
    "(16, 29),\n",
    "(17, 11),\n",
    "(17, 23),\n",
    "(17, 39),\n",
    "(17, 51),\n",
    "(18, 2),\n",
    "(18, 7),\n",
    "(18, 21),\n",
    "(19, 0),\n",
    "(19, 8),\n",
    "(19, 22),\n",
    "(19, 25),\n",
    "(19, 44),\n",
    "(0, 0),\n",
    "(0, 5),\n",
    "(0, 16),\n",
    "(0, 28),\n",
    "(1, 5),\n",
    "(1, 22),\n",
    "(1, 28),\n",
    "(1, 46),\n",
    "(2, 1),\n",
    "(2, 18),\n",
    "(2, 23),\n",
    "(3, 12),\n",
    "(3, 21),\n",
    "(3, 28),\n",
    "(4, 6),\n",
    "(4, 18),\n",
    "(4, 26),\n",
    "(4, 41),\n",
    "(5, 4),\n",
    "(5, 25),\n",
    "(5, 47),\n",
    "(5, 65),\n",
    "(6, 21),\n",
    "(6, 25),\n",
    "(9, 27),\n",
    "(9, 35),\n",
    "(9, 39),\n",
    "(10, 16),\n",
    "(10, 20),\n",
    "(10, 22)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_reshaped[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for id_key in add_list:\n",
    "    id = id_key[0]\n",
    "    fr = id_key[1]\n",
    "    ToPILImage()(predictions_reshaped[id]['prediction'][:,:,fr].astype(np.uint8)).save(data_path/'train'/'labels'/f'tst_{id}_{fr}_lab.png', format=\"PNG\")\n",
    "    ToPILImage()(test_data[id]['video'][:,:,:,fr].astype(np.uint8)).save(data_path/'train'/'scans'/f'tst_{id}_{fr}.png', format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_some_predictions(preds, test_data, ids, frames, 0.3, 1400)\n",
    "k = k + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_reshaped = predictions2list(preds, test_data, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "rsz = Resize(224, method=ResizeMethod.Crop)\n",
    "k = random.randint(0, 295)\n",
    "for i in range(k, k+6) :\n",
    "    pred_array = preds[0][i][1] > 0.2 # a threshold of 0.2 is applied to the prediction\n",
    "        \n",
    "    scan = test_data[ids[i]]['video'][:,:,frames[i]]\n",
    "    img = PILImageBW.create(scan)\n",
    "    ar = img.shape[0]/img.shape[1]\n",
    "    rsz = lambda o: CropPad(224)(RatioResize(224/ar)(o))\n",
    "\n",
    "    axs[(i-k)//3, (i-k)%3].imshow(rsz(img), cmap='gray', alpha=1)\n",
    "    axs[(i-k)//3, (i-k)%3].imshow(pred_array, alpha=0.5)\n",
    "    axs[(i-k)//3, (i-k)%3].set_title(f\"Image ID {ids[i]}, Frame {frames[i]}\")\n",
    "    axs[(i-k)//3, (i-k)%3].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predpath = Path(\"../out/predictions/\")\n",
    "predpath.mkdir(exist_ok=True, parents=True)\n",
    "save_zipped_pickle(predictions_reshaped, predpath/\"unet_224_amex_extended_tric_30pct_10ep.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data), len(predictions_reshaped)\n",
    "for i in range(len(test_data)):\n",
    "    print(test_data[i]['video'].shape == predictions_reshaped[i]['prediction'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
